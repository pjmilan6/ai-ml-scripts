{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71da216",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fb21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7a62bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f5015",
   "metadata": {},
   "source": [
    "### Non-streaming generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW is a variant of the Adam optimization algorithm that incorporates weight decay. It was introduced to address the issue of Adam's inability to generalize well when using weight decay. \n",
      "\n",
      "In AdamW, weight decay is applied directly to the model's weights, rather than incorporating it into the gradient update rule as a penalty term, which is the approach used in the original Adam algorithm. This modification allows AdamW to decouple weight decay from the adaptation of learning rates, resulting in more effective and efficient training of deep learning models.\n"
     ]
    }
   ],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=\"your-api-key\",\n",
    "    base_url=\"your-base-url\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model = 'Meta-Llama-3.3-70B-Instruct',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\", \"content\":\"Explain briefly the AdamW optimization algorithm.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45f462d",
   "metadata": {},
   "source": [
    "### Streaming generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0506b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model = 'Meta-Llama-3.3-70B-Instruct',\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\", \"content\":\"Explain the AdamW optimization algorithm in details.\"}\n",
    "    ],\n",
    "    stream=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "512d5de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW is a popular stochastic gradient descent optimization algorithm used for training deep learning models. It's an extension of the Adam algorithm, which is a widely used optimization algorithm in the field of deep learning. In this explanation, we'll dive into the details of the AdamW algorithm, its components, and how it works.\n",
      "\n",
      "**Introduction to Adam**\n",
      "\n",
      "Before we dive into AdamW, let's briefly review the Adam algorithm. Adam, which stands for Adaptive Moment Estimation, is a stochastic gradient descent optimization algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient. It was introduced by Kingma and Ba in 2014.\n",
      "\n",
      "Adam maintains two main components:\n",
      "\n",
      "1. **First moment estimate (m)**: This is the exponentially weighted moving average of the gradient.\n",
      "2. **Second moment estimate (v)**: This is the exponentially weighted moving average of the squared gradient.\n",
      "\n",
      "These two components are used to compute the adaptive learning rate for each parameter.\n",
      "\n",
      "**What is AdamW?**\n",
      "\n",
      "AdamW is an extension of the Adam algorithm that decouples weight decay from the learning rate. In traditional Adam, the weight decay is applied by adding a term to the gradient, which can lead to a suboptimal convergence. AdamW addresses this issue by applying the weight decay directly to the model weights, rather than modifying the gradient.\n",
      "\n",
      "**Key Components of AdamW**\n",
      "\n",
      "AdamW has the following key components:\n",
      "\n",
      "1. **First moment estimate (m)**: Same as in Adam, this is the exponentially weighted moving average of the gradient.\n",
      "2. **Second moment estimate (v)**: Same as in Adam, this is the exponentially weighted moving average of the squared gradient.\n",
      "3. **Weight decay (w)**: This is the decay rate applied directly to the model weights.\n",
      "4. **Learning rate (α)**: This is the step size used to update the model weights.\n",
      "\n",
      "**AdamW Update Rule**\n",
      "\n",
      "The AdamW update rule is as follows:\n",
      "\n",
      "1. Compute the gradient of the loss function with respect to the model weights: `g = ∇L(w)`\n",
      "2. Update the first moment estimate: `m = β1 * m + (1 - β1) * g`\n",
      "3. Update the second moment estimate: `v = β2 * v + (1 - β2) * g^2`\n",
      "4. Compute the adaptive learning rate: `α_t = α * sqrt(1 - β2^t) / (1 - β1^t)`\n",
      "5. Update the model weights: `w = w - α_t * (m / sqrt(v) + w * w)` (note the weight decay term `w * w`)\n",
      "\n",
      "**Decoupling Weight Decay**\n",
      "\n",
      "The key innovation in AdamW is the decoupling of weight decay from the learning rate. In traditional Adam, the weight decay is applied by adding a term to the gradient, which can lead to a suboptimal convergence. AdamW addresses this issue by applying the weight decay directly to the model weights, rather than modifying the gradient.\n",
      "\n",
      "**Benefits of AdamW**\n",
      "\n",
      "AdamW has several benefits, including:\n",
      "\n",
      "1. **Improved convergence**: AdamW can lead to improved convergence rates, especially in cases where the learning rate is large.\n",
      "2. **More stable training**: AdamW can help stabilize training by reducing the effect of large gradients.\n",
      "3. **Better generalization**: AdamW can lead to better generalization performance, especially in cases where the model is overfitting.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "In conclusion, AdamW is a popular stochastic gradient descent optimization algorithm that decouples weight decay from the learning rate. It's an extension of the Adam algorithm, which adapts the learning rate for each parameter based on the magnitude of the gradient. AdamW has several benefits, including improved convergence, more stable training, and better generalization performance. It's widely used in deep learning applications, including computer vision, natural language processing, and speech recognition."
     ]
    }
   ],
   "source": [
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quickstart_env (3.10.17)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
